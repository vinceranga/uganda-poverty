{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:18:52.056478",
     "start_time": "2018-04-04T00:18:50.879887"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.merge import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "# import imgaug as ia\n",
    "# from tqdm import tqdm\n",
    "# from imgaug import augmenters as iaa\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os, cv2\n",
    "# from preprocessing import parse_annotation, BatchGenerator\n",
    "from utils import WeightReader, decode_netout #, draw_boxes\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:18:52.075535",
     "start_time": "2018-04-04T00:18:52.057712"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LABELS = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "# LABELS = ['Fixed-wing Aircraft', 'Small Aircraft', 'Cargo Plane', 'Helicopter', 'Passenger Vehicle', 'Small Car', 'Bus', 'Pickup Truck', 'Utility Truck', 'Truck', 'Cargo Truck', 'Truck w/Box', 'Truck Tractor', 'Trailer', 'Truck w/Flatbed', 'Truck w/Liquid', 'Crane Truck', 'Railway Vehicle', 'Passenger Car', 'Cargo Car', 'Flat Car', 'Tank car', 'Locomotive', 'Maritime Vessel', 'Motorboat', 'Sailboat', 'Tugboat', 'Barge', 'Fishing Vessel', 'Ferry', 'Yacht', 'Container Ship', 'Oil Tanker', 'Engineering Vehicle', 'Tower crane', 'Container Crane', 'Reach Stacker', 'Straddle Carrier', 'Mobile Crane', 'Dump Truck', 'Haul Truck', 'Scraper/Tractor', 'Front loader/Bulldozer', 'Excavator', 'Cement Mixer', 'Ground Grader', 'Hut/Tent', 'Shed', 'Building', 'Aircraft Hangar', 'Damaged Building', 'Facility', 'Construction Site', 'Vehicle Lot', 'Helipad', 'Storage Tank', 'Shipping container lot', 'Shipping Container', 'Pylon', 'Tower']\n",
    "#LABELS = ['Small Car','Truck','Bus']\n",
    "LABELS=['Building']\n",
    "IMAGE_H, IMAGE_W = 416, 416\n",
    "GRID_H,  GRID_W  = 13, 13   # Vince: changed from 13 to 8\n",
    "BOX              = 5\n",
    "CLASS            = len(LABELS)\n",
    "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
    "OBJ_THRESHOLD    = 0.5#0.5\n",
    "NMS_THRESHOLD    = 0.2#0.45\n",
    "#ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "NO_OBJECT_SCALE  = 1.0\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "CLASS_SCALE      = 1.0\n",
    "\n",
    "BATCH_SIZE       = 16\n",
    "WARM_UP_BATCHES  = 0\n",
    "TRUE_BOX_BUFFER  = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:18:53.978220",
     "start_time": "2018-04-04T00:18:53.967537"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the function to implement the orgnization layer (thanks to github.com/allanzelener/YAD2K)\n",
    "def space_to_depth_x2(x):\n",
    "    return tf.space_to_depth(x, block_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T00:18:58.022959",
     "start_time": "2018-04-04T00:18:55.740759"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
    "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
    "\n",
    "# Layer 1\n",
    "x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
    "x = BatchNormalization(name='norm_1')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 2\n",
    "x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_2')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 3\n",
    "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_3')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 4\n",
    "x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_4')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 5\n",
    "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_5')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 6\n",
    "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_6')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 7\n",
    "x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_7')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 8\n",
    "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_8')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 9\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_9')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 10\n",
    "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_10')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 11\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_11')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 12\n",
    "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_12')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 13\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_13')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "skip_connection = x\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 14\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_14')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 15\n",
    "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_15')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 16\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_16')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 17\n",
    "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_17')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 18\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_18')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 19\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_19')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 20\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_20')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 21\n",
    "skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
    "skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
    "skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
    "skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
    "\n",
    "x = concatenate([skip_connection, x])\n",
    "\n",
    "# Layer 22\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_22')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 23\n",
    "x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
    "output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
    "\n",
    "# small hack to allow true_boxes to be registered when Keras build the model \n",
    "# for more information: https://github.com/fchollet/keras/issues/2790\n",
    "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
    "\n",
    "model = Model([input_image, true_boxes], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-26T12:34:03.819802Z",
     "start_time": "2017-11-26T12:34:03.786125Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the annotations to construct train generator and validation generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-26T12:38:44.283547Z",
     "start_time": "2017-11-26T12:38:44.277155Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator_config = {\n",
    "    'IMAGE_H'         : IMAGE_H, \n",
    "    'IMAGE_W'         : IMAGE_W,\n",
    "    'GRID_H'          : GRID_H,  \n",
    "    'GRID_W'          : GRID_W,\n",
    "    'BOX'             : BOX,\n",
    "    'LABELS'          : LABELS,\n",
    "    'CLASS'           : len(LABELS),\n",
    "    'ANCHORS'         : ANCHORS,\n",
    "    'BATCH_SIZE'      : BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER' : 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform detection on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T14:07:49.271978Z",
     "start_time": "2017-11-22T14:07:49.268999Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights_building.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper import IMG_ID as img_id\n",
    "from hyper import NUM_SQ\n",
    "import gc_util as gc\n",
    "\n",
    "sqs = [i for i in range(1, NUM_SQ+1)]\n",
    "gc.open_images(img_id, sqs)\n",
    "    \n",
    "img_files = 'LSMS_dg/dg_lsms_uganda_1000x1000_' + str(img_id) + '_*.jpeg'\n",
    "val_files = glob(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fn(name):\n",
    "    return int(name[42:-5])  # parsing out the sq variable\n",
    "    \n",
    "val_files = sorted(val_files, key=fn)\n",
    "val_files = val_files[:NUM_SQ]\n",
    "# for file in val_files: print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1156\n"
     ]
    }
   ],
   "source": [
    "print(len(val_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyper import OBJ_THRESH as obj_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyper import WH\n",
    "COORDS = [(i, j) for i in range(0,1000,WH) for j in range(0,1000,WH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def object_detection(val_files):\n",
    "    print(\"Performing object detection on %s tiles, each %s chips of w,h=%s\" % (NUM_SQ, int((1000/WH)**2), WH))\n",
    "    print()\n",
    "    \n",
    "    start = time.time()\n",
    "    for k, img_path in enumerate(val_files):\n",
    "        # check if file has already been scanned\n",
    "        \n",
    "        tst_name = 'boxes/' + img_path[8:-5] + '_' + str(int((1000/WH)**2) - 1) + '.pickle'\n",
    "        if os.path.isfile(tst_name):\n",
    "            continue\n",
    "        \n",
    "        if (k+1) % 10 == 0:\n",
    "            pdone = float(k) / len(val_files)\n",
    "            curr_time = time.time()\n",
    "            rem = int((1-pdone)/pdone * (curr_time - start))\n",
    "            print(\"%.2f percent done at sq %s. ETC: %s:%02d\" % (100 * pdone, k+1, int(rem / 60), rem % 60))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        # no longer resizing image here\n",
    "\n",
    "        for idx, coord_pair in enumerate(COORDS):\n",
    "            image_quarter = image[coord_pair[0]:coord_pair[0]+WH,\n",
    "                                  coord_pair[1]:coord_pair[1]+WH]\n",
    "\n",
    "            dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "            input_image = cv2.resize(image_quarter, (416, 416))\n",
    "            input_image = input_image / 255.\n",
    "            input_image = input_image[:,:,::-1]\n",
    "            input_image = np.expand_dims(input_image, 0)\n",
    "            netout = model.predict([input_image, dummy_array])\n",
    "\n",
    "            boxes = decode_netout(netout[0],\n",
    "                              obj_threshold=obj_thresh,\n",
    "                              nms_threshold=NMS_THRESHOLD,\n",
    "                              anchors=ANCHORS,\n",
    "                              nb_class=CLASS)\n",
    "\n",
    "            for box in boxes:\n",
    "                image_h, image_w, _ = image_quarter.shape\n",
    "\n",
    "                box.xmin *= image_w\n",
    "                box.ymin *= image_h\n",
    "                box.xmax *= image_w\n",
    "                box.ymax *= image_h\n",
    "\n",
    "            out_name = 'boxes/' + img_path[8:-5] + '_' + str(idx) + '.pickle'\n",
    "            pickle_out = open(out_name, \"wb\")\n",
    "            pickle.dump(boxes, pickle_out)\n",
    "            pickle_out.close()\n",
    "            \n",
    "    print()\n",
    "    print(\"Object detection completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing object detection on 1156 tiles, each 4 chips of w,h=500\n",
      "\n",
      "0.78 percent done at sq 10. ETC: 44:20\n",
      "1.64 percent done at sq 20. ETC: 43:53\n",
      "2.51 percent done at sq 30. ETC: 43:42\n",
      "3.37 percent done at sq 40. ETC: 43:39\n",
      "4.24 percent done at sq 50. ETC: 43:17\n",
      "5.10 percent done at sq 60. ETC: 43:01\n",
      "5.97 percent done at sq 70. ETC: 42:44\n",
      "6.83 percent done at sq 80. ETC: 42:21\n",
      "7.70 percent done at sq 90. ETC: 41:56\n",
      "8.56 percent done at sq 100. ETC: 41:41\n",
      "9.43 percent done at sq 110. ETC: 41:19\n",
      "10.29 percent done at sq 120. ETC: 40:55\n",
      "11.16 percent done at sq 130. ETC: 40:32\n",
      "12.02 percent done at sq 140. ETC: 40:08\n",
      "12.89 percent done at sq 150. ETC: 39:45\n",
      "13.75 percent done at sq 160. ETC: 39:21\n",
      "14.62 percent done at sq 170. ETC: 38:58\n",
      "15.48 percent done at sq 180. ETC: 38:34\n",
      "16.35 percent done at sq 190. ETC: 38:11\n",
      "17.21 percent done at sq 200. ETC: 37:48\n",
      "18.08 percent done at sq 210. ETC: 37:25\n",
      "18.94 percent done at sq 220. ETC: 37:02\n",
      "19.81 percent done at sq 230. ETC: 36:39\n",
      "20.67 percent done at sq 240. ETC: 36:16\n",
      "21.54 percent done at sq 250. ETC: 35:54\n",
      "22.40 percent done at sq 260. ETC: 35:32\n",
      "23.27 percent done at sq 270. ETC: 35:09\n",
      "24.13 percent done at sq 280. ETC: 34:47\n",
      "25.00 percent done at sq 290. ETC: 34:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinceranga/Documents/poverty/utils.py:222: RuntimeWarning: overflow encountered in exp\n",
      "  return 1. / (1. + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.87 percent done at sq 300. ETC: 34:02\n",
      "26.73 percent done at sq 310. ETC: 33:39\n",
      "27.60 percent done at sq 320. ETC: 33:17\n",
      "28.46 percent done at sq 330. ETC: 32:54\n",
      "29.33 percent done at sq 340. ETC: 32:31\n",
      "30.19 percent done at sq 350. ETC: 32:08\n",
      "31.06 percent done at sq 360. ETC: 31:44\n",
      "31.92 percent done at sq 370. ETC: 31:21\n",
      "32.79 percent done at sq 380. ETC: 30:58\n",
      "33.65 percent done at sq 390. ETC: 30:34\n",
      "34.52 percent done at sq 400. ETC: 30:11\n",
      "35.38 percent done at sq 410. ETC: 29:48\n",
      "36.25 percent done at sq 420. ETC: 29:24\n",
      "37.11 percent done at sq 430. ETC: 29:01\n",
      "37.98 percent done at sq 440. ETC: 28:37\n",
      "38.84 percent done at sq 450. ETC: 28:14\n",
      "39.71 percent done at sq 460. ETC: 27:50\n",
      "40.57 percent done at sq 470. ETC: 27:27\n",
      "41.44 percent done at sq 480. ETC: 27:03\n",
      "42.30 percent done at sq 490. ETC: 26:40\n",
      "43.17 percent done at sq 500. ETC: 26:16\n",
      "44.03 percent done at sq 510. ETC: 25:53\n",
      "44.90 percent done at sq 520. ETC: 25:29\n",
      "45.76 percent done at sq 530. ETC: 25:05\n",
      "46.63 percent done at sq 540. ETC: 24:41\n",
      "47.49 percent done at sq 550. ETC: 24:18\n",
      "48.36 percent done at sq 560. ETC: 23:54\n",
      "49.22 percent done at sq 570. ETC: 23:31\n",
      "50.09 percent done at sq 580. ETC: 23:07\n",
      "50.95 percent done at sq 590. ETC: 22:43\n",
      "51.82 percent done at sq 600. ETC: 22:19\n",
      "52.68 percent done at sq 610. ETC: 21:56\n",
      "53.55 percent done at sq 620. ETC: 21:32\n",
      "54.41 percent done at sq 630. ETC: 21:08\n",
      "55.28 percent done at sq 640. ETC: 20:43\n",
      "56.14 percent done at sq 650. ETC: 20:19\n",
      "57.01 percent done at sq 660. ETC: 19:55\n",
      "57.87 percent done at sq 670. ETC: 19:31\n",
      "58.74 percent done at sq 680. ETC: 19:07\n",
      "59.60 percent done at sq 690. ETC: 18:43\n",
      "60.47 percent done at sq 700. ETC: 18:19\n",
      "61.33 percent done at sq 710. ETC: 17:55\n",
      "62.20 percent done at sq 720. ETC: 17:31\n",
      "63.06 percent done at sq 730. ETC: 17:07\n",
      "63.93 percent done at sq 740. ETC: 16:43\n",
      "64.79 percent done at sq 750. ETC: 16:19\n",
      "65.66 percent done at sq 760. ETC: 15:55\n",
      "66.52 percent done at sq 770. ETC: 15:31\n",
      "67.39 percent done at sq 780. ETC: 15:07\n",
      "68.25 percent done at sq 790. ETC: 21:15\n",
      "69.12 percent done at sq 800. ETC: 32:53\n",
      "69.98 percent done at sq 810. ETC: 32:03\n",
      "70.85 percent done at sq 820. ETC: 31:08\n",
      "71.71 percent done at sq 830. ETC: 30:11\n",
      "72.58 percent done at sq 840. ETC: 29:13\n",
      "73.44 percent done at sq 850. ETC: 28:16\n",
      "74.31 percent done at sq 860. ETC: 27:19\n",
      "75.17 percent done at sq 870. ETC: 26:22\n",
      "76.04 percent done at sq 880. ETC: 25:25\n",
      "76.90 percent done at sq 890. ETC: 24:29\n",
      "77.77 percent done at sq 900. ETC: 23:32\n",
      "78.63 percent done at sq 910. ETC: 22:37\n",
      "79.50 percent done at sq 920. ETC: 21:41\n",
      "80.36 percent done at sq 930. ETC: 20:45\n",
      "81.23 percent done at sq 940. ETC: 19:49\n",
      "82.09 percent done at sq 950. ETC: 18:53\n",
      "82.96 percent done at sq 960. ETC: 17:58\n",
      "83.82 percent done at sq 970. ETC: 17:03\n",
      "84.69 percent done at sq 980. ETC: 16:07\n",
      "85.55 percent done at sq 990. ETC: 15:12\n",
      "86.42 percent done at sq 1000. ETC: 14:16\n",
      "87.28 percent done at sq 1010. ETC: 13:22\n",
      "88.15 percent done at sq 1020. ETC: 12:27\n",
      "89.01 percent done at sq 1030. ETC: 11:32\n",
      "89.88 percent done at sq 1040. ETC: 10:37\n",
      "90.74 percent done at sq 1050. ETC: 9:42\n",
      "91.61 percent done at sq 1060. ETC: 8:47\n",
      "92.47 percent done at sq 1070. ETC: 7:53\n",
      "93.34 percent done at sq 1080. ETC: 6:58\n",
      "94.20 percent done at sq 1090. ETC: 6:03\n",
      "95.07 percent done at sq 1100. ETC: 5:09\n",
      "95.93 percent done at sq 1110. ETC: 4:14\n",
      "96.80 percent done at sq 1120. ETC: 3:21\n",
      "97.66 percent done at sq 1130. ETC: 2:28\n",
      "98.53 percent done at sq 1140. ETC: 1:34\n",
      "99.39 percent done at sq 1150. ETC: 0:38\n",
      "\n",
      "Object detection completed!\n"
     ]
    }
   ],
   "source": [
    "object_detection(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper import HOUSE_SIZE_THRESH\n",
    "from hyper import HOUSE_LEN_THRESH\n",
    "\n",
    "def draw_boxes_coords(img_id, sq, coords=COORDS, filter_size=True, filter_green=True):\n",
    "    img_path = \"LSMS_dg/dg_lsms_uganda_1000x1000_\" + str(img_id) + \"_\" + str(sq) + \".jpeg\"\n",
    "    \n",
    "    imgs = []\n",
    "    \n",
    "    if sq % 50 == 1:\n",
    "        print(\"Currently at sq %d\" %sq)\n",
    "    \n",
    "    for idx, coord_pair in enumerate(coords):\n",
    "        pickle_path = \"boxes/dg_lsms_uganda_1000x1000_\" + str(img_id) + \"_\" + str(sq) + \"_\" + str(idx) + \".pickle\"\n",
    "\n",
    "        pickle_in = open(pickle_path, \"rb\")\n",
    "        boxes = pickle.load(pickle_in)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = img[coord_pair[0]:coord_pair[0]+WH,\n",
    "                  coord_pair[1]:coord_pair[1]+WH]\n",
    "\n",
    "        for tbox in boxes:\n",
    "            if filter_size:\n",
    "                size = (tbox.xmax - tbox.xmin) * (tbox.ymax - tbox.ymin)\n",
    "                def bad_house_size(tbox):\n",
    "                    return size > HOUSE_SIZE_THRESH or (tbox.xmax - tbox.xmin) > HOUSE_LEN_THRESH or (tbox.ymax - tbox.ymin) > HOUSE_LEN_THRESH\n",
    "                if bad_house_size(tbox): continue\n",
    "            \n",
    "            p1 = (int(tbox.xmin), int(tbox.ymin))\n",
    "            p2 = (int(tbox.xmax), int(tbox.ymax))\n",
    "            \n",
    "            if filter_green:\n",
    "                reg = np.array(img[p1[1]:p2[1], p1[0]:p2[0]])\n",
    "                # plt.imshow(reg)\n",
    "                # plt.show()\n",
    "                \n",
    "                def bad_green_maj(reg, thresh=0.70):\n",
    "                    num_pix = (p2[1] - p1[1]) * (p2[0] - p1[0])\n",
    "                    green = reg[:,:,1]\n",
    "                    mxd = np.amax(reg, axis=2)\n",
    "                    num_nongreen = np.count_nonzero(green - mxd)\n",
    "                    pc = 1 - float(num_nongreen) / num_pix\n",
    "                    # print(pc)\n",
    "                    return pc > thresh\n",
    "                \n",
    "                if bad_green_maj(reg): continue\n",
    "                \n",
    "            img = cv2.rectangle(img, p1, p2, (255,0,0), thickness=1)\n",
    "    \n",
    "        imgs.append(img)\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_boxes_coords(img_id, sq, coords=COORDS, filter_size=True, filter_green=True):\n",
    "    imgs = draw_boxes_coords(img_id, sq, coords=COORDS, filter_size=filter_size, filter_green=filter_green)\n",
    "    \n",
    "    if WH == 250:\n",
    "        rws = []\n",
    "        for k in range(0,16,4):\n",
    "            rws.append(np.concatenate([imgs[k], imgs[k+1], imgs[k+2], imgs[k+3]], axis=1))\n",
    "        img = np.concatenate([rws[0], rws[1], rws[2], rws[3]], axis=0)\n",
    "        path = \"box_imgs/dg_lsms_uganda_1000x1000_%s_%s_boxed.jpeg\" % (img_id, sq)\n",
    "        cv2.imwrite(path, img)\n",
    "    elif WH == 500:\n",
    "        for idx in range(len(coords)):\n",
    "            path = \"box_imgs/dg_lsms_uganda_1000x1000_\" + str(img_id) + \"_\" + str(sq) + \"_\" + str(idx) + \".jpeg\"\n",
    "            cv2.imwrite(path, imgs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at sq 1\n",
      "Currently at sq 51\n",
      "Currently at sq 101\n",
      "Currently at sq 151\n",
      "Currently at sq 201\n",
      "Currently at sq 251\n",
      "Currently at sq 301\n",
      "Currently at sq 351\n",
      "Currently at sq 401\n",
      "Currently at sq 451\n",
      "Currently at sq 501\n",
      "Currently at sq 551\n",
      "Currently at sq 601\n",
      "Currently at sq 651\n",
      "Currently at sq 701\n",
      "Currently at sq 751\n",
      "Currently at sq 801\n",
      "Currently at sq 851\n",
      "Currently at sq 901\n",
      "Currently at sq 951\n",
      "Currently at sq 1001\n",
      "Currently at sq 1051\n",
      "Currently at sq 1101\n",
      "Currently at sq 1151\n"
     ]
    }
   ],
   "source": [
    "for sq in range(1, NUM_SQ+1):\n",
    "    save_boxes_coords(img_id, sq, filter_size=True, filter_green=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete folders for restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def empty_folder(folder):\n",
    "    import shutil\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_folder(\"box_imgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_folder(\"boxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_folder(\"LSMS_dg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {
    "height": "122px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "758px",
    "left": "0px",
    "right": "1096px",
    "top": "73px",
    "width": "253px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
